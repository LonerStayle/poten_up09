{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6156a903",
   "metadata": {
    "id": "6156a903"
   },
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê³  ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b59784",
   "metadata": {
    "id": "e0b59784"
   },
   "outputs": [],
   "source": [
    "# uv add huggingface_hub\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0e979",
   "metadata": {
    "id": "d1c0e979",
    "outputId": "1b4eb382-3572-4298-fac9-32a0cf1d7e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081874bc",
   "metadata": {
    "id": "081874bc"
   },
   "source": [
    "# 1) Gemma3 1B it ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ab057",
   "metadata": {
    "id": "1e8ab057"
   },
   "source": [
    "### pipelineìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f625d",
   "metadata": {
    "id": "f69f625d",
    "outputId": "8457c60b-5dab-43ad-f9e4-fccf5361b235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}, {'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ë„ ë”°ëœ»í•˜ê³  í–‰ë³µí•˜ê²Œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*'}]}]\n"
     ]
    }
   ],
   "source": [
    "# uv add transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ì…ë ¥ê°’ ë§Œë“¤ê¸°\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”\"\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9d915",
   "metadata": {
    "id": "78c9d915"
   },
   "outputs": [],
   "source": [
    "# ì •ë¦¬ 1. outputì€ ì–´ë–»ê²Œ ìƒê²¼ì„ê¹Œ\n",
    "# ì •ë¦¬ 2. messages í˜•íƒœëŠ” ì™œ ì´ë ‡ê²Œ ì´ë£¨ì–´ì ¸ ìˆì„ê¹Œ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6c73f",
   "metadata": {
    "id": "26b6c73f",
    "outputId": "ec267b35-43e0-4f06-ba29-5f93b6e25845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}, {'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ë„ ë”°ëœ»í•˜ê³  í–‰ë³µí•˜ê²Œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*'}]}\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}, {'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ë„ ë”°ëœ»í•˜ê³  í–‰ë³µí•˜ê²Œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*'}]\n",
      "{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}\n",
      "{'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ë„ ë”°ëœ»í•˜ê³  í–‰ë³µí•˜ê²Œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*'}\n"
     ]
    }
   ],
   "source": [
    "# ë¯¸ì…˜ output íŒŒì‹±í•´ë³´ê¸°\n",
    "for out in output:\n",
    "    print(out)\n",
    "    print(out[\"generated_text\"])\n",
    "    for x in out[\"generated_text\"]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dada1cf",
   "metadata": {
    "id": "4dada1cf",
    "outputId": "6d1e5798-e4df-4f28-f0d0-7fca3920f2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}, {'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ ë“œë¦´ê²Œìš”. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \\n\\n*   **ì‘ì€ ê²ƒ'}]}]]\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥ê°’ ë§Œë“¤ê¸°\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d238e",
   "metadata": {
    "id": "be9d238e"
   },
   "source": [
    "### ëª¨ë¸ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f0cd7",
   "metadata": {
    "id": "162f0cd7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa3001",
   "metadata": {
    "id": "68aa3001"
   },
   "outputs": [],
   "source": [
    "test_dict = {\"device\": \"cuda\", \"verbose\": True}\n",
    "\n",
    "temp_func(**test_dict)\n",
    "\n",
    "temp_func(device=\"cuda\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29acb50",
   "metadata": {
    "id": "d29acb50"
   },
   "outputs": [],
   "source": [
    "# uv add accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7203014",
   "metadata": {
    "id": "a7203014",
    "outputId": "96b2dffb-d553-4d16-857e-7a8daaf93f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
      "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
      "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 1. (add_generation_prompt=False, tokenize=False)\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 2. (add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputë’¤ì—ë‹¤ê°€ assistantë¥¼ ë¶™ì¼ì§€ ê²°ì •\n",
    "    tokenize=True,              # ê²°ê³¼ë¥¼ í† í°í™”í• ì§€ ì—¬ë¶€\n",
    "    return_dict=True,           # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ê²Œ í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    "    return_tensors=\"pt\",        # ê²°ê³¼ë¥¼ íŒŒì´í† ì¹˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    ")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd95bd",
   "metadata": {
    "id": "3abd95bd"
   },
   "outputs": [],
   "source": [
    "# ì •ë¦¬1. messagesë³€ìˆ˜ê°€ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë‚˜ ìˆ«ìë¡œ ë°”ë€ŒëŠ” ê³¼ì •ì€ ì–´ë–¤ê°€?\n",
    "messages = [\n",
    "    {},\n",
    "    {}\n",
    "]\n",
    "# ì •ë¦¬2. ìš°ë¦¬ê°€ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì–´ì•¼ í• ê¹Œ?\n",
    "# ì •ë¦¬3. ì˜ˆì¸¡ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ \"**\"ëŠ” ì™œ ì“°ëŠ”ê±¸ê¹Œ?\n",
    "# ì •ë¦¬4. outputì€ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ê°€?\n",
    "# ì •ë¦¬5. ouutputì—ì„œ ë‹µë³€ì€ ì–´ë–»ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆì„ê¹Œ?\n",
    "\n",
    "## messagesë¥¼ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ìˆ«ìë¡œ ë°”ê¿”ì•¼ í•œë‹¤!\n",
    "## tokenizer.apply_chat_templateì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì´ìš©, tokenize=False ---- messagesê°€ ë¬¸ìë¡œ ë°”ë€ ìƒíƒœê°€ ë‚˜ì˜¨ë‹¤.\n",
    "## tokenize=Trueë¡œ ì„¤ì •í•˜ê²Œ ë˜ë©´, input_ids, attention_mask ë”•ì…”ë„ˆë¦¬ë¡œ ì¶œë ¥ëœë‹¤.\n",
    "## input_ids : ë¬¸ì -> ìˆ«ì , attention_mask: ê·¸ ìë¦¬ê°€ ì˜ë¯¸ê°€ ìˆëŠ” ìë¦¬ì¸ì§€ ì˜ë¯¸ê°€ ì—†ëŠ” ìë¦¬ì¸ì§€ë¥¼ ì•Œë ¤ì¤Œ\n",
    "## ì´ì œ inputsê°€ ì¤€ë¹„ë˜ì—ˆë‹¤ -- GPUì— ì˜®ê¸´ë‹¤ to(device) -- model.generate()\n",
    "## outputsê°€ ë‚˜ì˜¨ë‹¤. ì´ ì¹œêµ¬ëŠ” ì–´ë–»ê²Œ ìƒê²¼ì„ê¹Œ? ë‹µë³€ì´ ì–´ë””ìˆëŠ”ì§€ ì°¾ì„ ìˆ˜ ìˆëŠ”ê°€??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7a8fd",
   "metadata": {
    "id": "fab7a8fd",
    "outputId": "745ceaf8-0d80-4c9f-a601-0b03db36f16d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False, # inputë’¤ì—ë‹¤ê°€ assistantë¥¼ ë¶™ì¼ì§€ ê²°ì •\n",
    "    tokenize=False,              # ê²°ê³¼ë¥¼ í† í°í™”í• ì§€ ì—¬ë¶€\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a54922",
   "metadata": {
    "id": "60a54922",
    "outputId": "5e084172-03c7-4611-83eb-7ea6f3243ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputë’¤ì—ë‹¤ê°€ assistantë¥¼ ë¶™ì¼ì§€ ê²°ì •\n",
    "    tokenize=False,              # ê²°ê³¼ë¥¼ í† í°í™”í• ì§€ ì—¬ë¶€\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c15aca",
   "metadata": {
    "id": "58c15aca",
    "outputId": "a6ce3e65-2769-470a-f66d-4cdf2a9e7083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d366e5",
   "metadata": {
    "id": "15d366e5"
   },
   "outputs": [],
   "source": [
    "model.generate(input_ids, attention_mask, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c2cd2",
   "metadata": {
    "id": "109c2cd2",
    "outputId": "8b7afcc0-bf74-4892-fc3e-47349cfa5344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\nWrite a poem on Hugging Face, the company<end_of_turn>\\n<start_of_turn>model\\nOkay, hereâ€™s a poem about Hugging Face, aiming to capture its essence â€“ a community of AI, collaboration, and open-source creativity:\\n\\n**The Hugging Face Bloom**\\n\\nThe cloud expands, a gentle hue,\\nHugging Face, a vibrant view.\\nA haven built for code and']\n"
     ]
    }
   ],
   "source": [
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8f522",
   "metadata": {
    "id": "d8b8f522",
    "outputId": "8af26b48-4bcb-4ea7-9a8f-76805d8a9c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, hereâ€™s a poem about Hugging Face, aiming to capture its essence â€“ a community of AI, collaboration, and open-source creativity:\n",
      "\n",
      "**The Hugging Face Bloom**\n",
      "\n",
      "The cloud expands, a gentle hue,\n",
      "Hugging Face, a vibrant view.\n",
      "A haven built for code and\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d693d",
   "metadata": {
    "id": "903d693d",
    "outputId": "c1efbadc-67d7-449a-c61c-120a38b28127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ff73d",
   "metadata": {
    "id": "6b9ff73d",
    "outputId": "ec20887d-d998-4ef0-de28-ec446aeaae6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ou are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, hereâ€™s a poem about Hugging Face, aiming to capture its essence â€“ a community of AI, collaboration, and open-source creativity:\n",
      "\n",
      "**The Hugging Face Bloom**\n",
      "\n",
      "The cloud expands, a gentle hue,\n",
      "Hugging Face, a vibrant view.\n",
      "A haven built for code and\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputë’¤ì—ë‹¤ê°€ assistantë¥¼ ë¶™ì¼ì§€ ê²°ì •\n",
    "    tokenize=True,              # ê²°ê³¼ë¥¼ í† í°í™”í• ì§€ ì—¬ë¶€\n",
    "    return_dict=True,           # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ê²Œ í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    "    return_tensors=\"pt\",        # ê²°ê³¼ë¥¼ íŒŒì´í† ì¹˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    ")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "print(outputs[0][input_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4aaaf",
   "metadata": {},
   "source": [
    "## 2) Gemma3 1B pt ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8393e6",
   "metadata": {},
   "source": [
    "### pipelineìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b5f29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-pt\", \n",
    "    device=device, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "output = pipe(\"Eiffel tower is located in\", max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a73e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the worldâ€™s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located'}]\n",
      "{'generated_text': 'Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the worldâ€™s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located'}\n",
      "Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the worldâ€™s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(output[0])\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8a3b6",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, ì½”íŠ¸ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(device) # GPU ë³´ë‚´ê¸°\n",
    "\n",
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.decode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7f5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, ì½”íŠ¸ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c960387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU ë³´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5c5dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "torch.Size([1, 57])\n",
      "tensor([     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "           529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "           563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "         11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "        236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "          9079,    532,   7001], device='cuda:0')\n",
      "<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "print(outputs)\n",
    "print(outputs.shape)\n",
    "print(outputs[0])\n",
    "\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5927d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf22519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë¦¬\n",
    "# 1. ìš°ë¦¬ê°€ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ, ëª¨ë¸ì€ ì–´ë–»ê²Œ ë¶ˆëŸ¬ì˜¤ë‚˜ìš”?\n",
    "# 2. instruct ëª¨ë¸ì´ ìˆê³ , ê·¸ëƒ¥ pre-trained ëª¨ë¸ì´ ìˆëŠ”ë° ëª¨ë¸ì— inputí•´ì•¼ í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "# 3. input í•´ì•¼í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ë§Œë“œë‚˜ìš”?\n",
    "# 4. modelì—ì„œ input dataë¥¼ ë„£ì€ ë‹¤ìŒ ë‚˜ì˜¨ outputì€ ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "# 5. outputì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22f99f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 239120, 238500, 238503, 239592, 237170, 110388, 237223,   5386,\n",
      "         103595, 236881,    107, 238136, 239484, 241845, 237456, 110388, 237223,\n",
      "           5386, 103595, 236881]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# ì‘ìš©í•´ë³´ê¸°(ì—…ë°ì´íŠ¸ ì¤‘ì…ë‹ˆë‹¤.......)\n",
    "prompt = [\n",
    "    \"ë‚¨ì‚°íƒ€ì›ŒëŠ” ì–´ë””ì— ìˆë‚˜ìš”?<eos>\",\n",
    "    \"ê²½ë³µê¶ì€ ì–´ë””ì— ìˆë‚˜ìš”?<eos>\"\n",
    "]\n",
    "prompt = \"ë‚¨ì‚°íƒ€ì›ŒëŠ” ì–´ë””ì— ìˆë‚˜ìš”?\\nê²½ë³µê¶ì€ ì–´ë””ì— ìˆë‚˜ìš”?\"\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(\n",
    "    \"\".join(prompt), \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU ë³´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemme-3 1b it\n",
    "\n",
    "# ì •ë¦¬1. messagesë³€ìˆ˜ê°€ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë‚˜ ìˆ«ìë¡œ ë°”ë€ŒëŠ” ê³¼ì •ì€ ì–´ë–¤ê°€?\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "## ëª¨ë¸ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆëŠ”ê°€????? No\n",
    "## messages ë¥¼ ìˆ«ì í…ì„œë¡œ ë°”ê¿”ì•¼í•œë‹¤. ---- How?? tokenize.apply_chat_template(message)\n",
    "## ë§Œì•½ tokenize=Trueë¥¼ í•˜ë©´, {\"input_ids\":, \"attention_mask\":}\n",
    "## ??? messages ëŠ” ë¬¸ìê°€ ì•„ë‹Œë° ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¾¸ëŠ”ê±°ì§€?????? tokenize = False í•´ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.\n",
    "##  <bos><start....... bos ëŠ” ì‹œì‘, eos ë\n",
    "\n",
    "# ì •ë¦¬2. ìš°ë¦¬ê°€ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì–´ì•¼ í• ê¹Œ?\n",
    "## inputs = {\"input_ids\":\"A\", \"attention_mask\":\"B\"}\n",
    "\n",
    "# ì •ë¦¬3. ì˜ˆì¸¡ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ \"**\"ëŠ” ì™œ ì“°ëŠ”ê±¸ê¹Œ?\n",
    "## def myfunc(input_ids, attention_mask, message=\"CCC\", verbose = Fase) \n",
    "## ...\n",
    "## myfunc(**inputs)ì˜ ì˜ë¯¸ëŠ” myfunc(input_ids = \"A\", attention_mask=\"B\") ë¡œ í•´ì£¼ì„¸ìš”ì™€ ê°™ë‹¤.\n",
    "\n",
    "# ì •ë¦¬4. outputì€ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ê°€?\n",
    "## outputs = model(inputs)\n",
    "## tensor - ìˆ«ìí˜•\n",
    "\n",
    "# ì •ë¦¬5. ouutputì—ì„œ ë‹µë³€ì€ ì–´ë–»ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆì„ê¹Œ? \n",
    "# decodeë¥¼ í†µí•´ì„œ ìˆ«ì í…ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°”ê¿©í•œë‹¤.\n",
    "# tensor([ë°ì´í„°1, ë°ì´í„°2 ...]) ì¸ ê²½ìš°, tokenizer.batch_decode\n",
    "# tensor(ë°ì´í„°1)ì¸ ê²½ìš°, tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-3 1b pt\n",
    "\n",
    "# ì •ë¦¬\n",
    "# 1. ìš°ë¦¬ê°€ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ, ëª¨ë¸ì€ ì–´ë–»ê²Œ ë¶ˆëŸ¬ì˜¤ë‚˜ìš”?\n",
    "## ëª¨ë¸ ì´ë¦„ì´ ìˆëŠ” ì‚¬ì´íŠ¸ì— ë“¤ì–´ê°€ë©´ \n",
    "## pipeline í•¨ìˆ˜ë¡œë„ ì¶”ë¡ í•  ìˆ˜ ìˆê³ ,\n",
    "## modelì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ì„œ í•  ìˆ˜ë„ ìˆë‹¤.----(ë³„í‘œ) íŒŒì¸íŠœë‹: ë‚´ ë°ì´í„°ë¥¼ ì´ë¯¸ í•™ìŠµë˜ì–´ ìˆëŠ” ëª¨ë¸ì— ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ model \n",
    "\n",
    "# 2. instruct ëª¨ë¸ì´ ìˆê³ , ê·¸ëƒ¥ pre-trained ëª¨ë¸ì´ ìˆëŠ”ë° ëª¨ë¸ì— inputí•´ì•¼ í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "## LLM í•™ìŠµ ë°©ë²• \n",
    "## 1) Pre-traind model ì—ì„œ \"ìƒˆë¡œìš´ ì •ë³´\"ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ex. gemma-3-1b-pt INPUT: \"í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”\" (str)\n",
    "## 2) Instruct model ì—ì„œ \"ë§í•˜ëŠ” ë°©ì‹\"ì„ í•™ìŠµì‹œí‚¨ë‹¤. ex. gemma-3-1b-it INPUT: ëŒ€í™” [{\"role\":,\"content\":}] * ì§€ì‹œì‚¬í•­ê¹Œì§€ í•™ìŠµí•œë‹¤.\n",
    "\n",
    "# 3. input í•´ì•¼í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ë§Œë“œë‚˜ìš”?\n",
    "## í…ìŠ¤íŠ¸ -- í…ì„œ -- model -- í…ìŠ¤íŠ¸\n",
    "##    tokenizer       tokenizer\n",
    "##    tokenize()      tokenizer.decode()\n",
    "##    tokenize.apply_chat_template()  \n",
    "\n",
    "# 4. modelì—ì„œ input dataë¥¼ ë„£ì€ ë‹¤ìŒ ë‚˜ì˜¨ outputì€ ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "## tenser \n",
    "\n",
    "# 5. outputì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n",
    "# tokenizer.decode ì‹œë¦¬ì¦ˆ í™œìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c414730",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "poten_up09",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
