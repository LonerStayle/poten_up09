{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6156a903",
   "metadata": {
    "id": "6156a903"
   },
   "source": [
    "# 모델 불러오고 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b59784",
   "metadata": {
    "id": "e0b59784"
   },
   "outputs": [],
   "source": [
    "# uv add huggingface_hub\n",
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0e979",
   "metadata": {
    "id": "d1c0e979",
    "outputId": "1b4eb382-3572-4298-fac9-32a0cf1d7e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081874bc",
   "metadata": {
    "id": "081874bc"
   },
   "source": [
    "# 1) Gemma3 1B it 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ab057",
   "metadata": {
    "id": "1e8ab057"
   },
   "source": [
    "### pipeline으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f625d",
   "metadata": {
    "id": "f69f625d",
    "outputId": "8457c60b-5dab-43ad-f9e4-fccf5361b235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}, {'role': 'assistant', 'content': '안녕하세요! 오늘 하루도 따뜻하고 행복하게 보내시길 바랍니다. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*'}]}]\n"
     ]
    }
   ],
   "source": [
    "# uv add transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 파이프라인 만들기\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 입력값 만들기\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"친절하게 말하세요\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"오늘 하루를 응원해주세요\"\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"친절하게 말하세요\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"오늘 하루를 응원해주세요\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9d915",
   "metadata": {
    "id": "78c9d915"
   },
   "outputs": [],
   "source": [
    "# 정리 1. output은 어떻게 생겼을까\n",
    "# 정리 2. messages 형태는 왜 이렇게 이루어져 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6c73f",
   "metadata": {
    "id": "26b6c73f",
    "outputId": "ec267b35-43e0-4f06-ba29-5f93b6e25845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}, {'role': 'assistant', 'content': '안녕하세요! 오늘 하루도 따뜻하고 행복하게 보내시길 바랍니다. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*'}]}\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}, {'role': 'assistant', 'content': '안녕하세요! 오늘 하루도 따뜻하고 행복하게 보내시길 바랍니다. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*'}]\n",
      "{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}\n",
      "{'role': 'assistant', 'content': '안녕하세요! 오늘 하루도 따뜻하고 행복하게 보내시길 바랍니다. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*'}\n"
     ]
    }
   ],
   "source": [
    "# 미션 output 파싱해보기\n",
    "for out in output:\n",
    "    print(out)\n",
    "    print(out[\"generated_text\"])\n",
    "    for x in out[\"generated_text\"]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dada1cf",
   "metadata": {
    "id": "4dada1cf",
    "outputId": "6d1e5798-e4df-4f28-f0d0-7fca3920f2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}, {'role': 'assistant', 'content': '안녕하세요! 😊 오늘 하루를 응원해 드릴게요. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내시길 바랍니다. \\n\\n*   **작은 것'}]}]]\n"
     ]
    }
   ],
   "source": [
    "# 입력값 만들기\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"친절하게 말하세요\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"오늘 하루를 응원해주세요\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d238e",
   "metadata": {
    "id": "be9d238e"
   },
   "source": [
    "### 모델로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f0cd7",
   "metadata": {
    "id": "162f0cd7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# 모델 이름 설정\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. 입력 데이터 준비하기\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# STEP4. 추론하기\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa3001",
   "metadata": {
    "id": "68aa3001"
   },
   "outputs": [],
   "source": [
    "test_dict = {\"device\": \"cuda\", \"verbose\": True}\n",
    "\n",
    "temp_func(**test_dict)\n",
    "\n",
    "temp_func(device=\"cuda\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29acb50",
   "metadata": {
    "id": "d29acb50"
   },
   "outputs": [],
   "source": [
    "# uv add accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# 모델 이름 설정\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7203014",
   "metadata": {
    "id": "a7203014",
    "outputId": "96b2dffb-d553-4d16-857e-7a8daaf93f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
      "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
      "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. 입력 데이터 준비하기\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "# 테스트해보기 1. (add_generation_prompt=False, tokenize=False)\n",
    "# 테스트해보기 2. (add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # input뒤에다가 assistant를 붙일지 결정\n",
    "    tokenize=True,              # 결과를 토큰화할지 여부\n",
    "    return_dict=True,           # 결과를 딕셔너리로 반환하게 할 것인지의 여부\n",
    "    return_tensors=\"pt\",        # 결과를 파이토치 형식으로 반환할 것인지의 여부\n",
    ")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd95bd",
   "metadata": {
    "id": "3abd95bd"
   },
   "outputs": [],
   "source": [
    "# 정리1. messages변수가 토크나이저를 만나 숫자로 바뀌는 과정은 어떤가?\n",
    "messages = [\n",
    "    {},\n",
    "    {}\n",
    "]\n",
    "# 정리2. 우리가 예측을 하기 위해서는 어떤 데이터가 준비되어야 할까?\n",
    "# 정리3. 예측을 하는 과정에서 \"**\"는 왜 쓰는걸까?\n",
    "# 정리4. output은 어떻게 나오는가?\n",
    "# 정리5. ouutput에서 답변은 어떻게 추출할 수 있을까?\n",
    "\n",
    "## messages를 토크나이저를 통해 숫자로 바꿔야 한다!\n",
    "## tokenizer.apply_chat_template이라는 함수를 이용, tokenize=False ---- messages가 문자로 바뀐 상태가 나온다.\n",
    "## tokenize=True로 설정하게 되면, input_ids, attention_mask 딕셔너리로 출력된다.\n",
    "## input_ids : 문자 -> 숫자 , attention_mask: 그 자리가 의미가 있는 자리인지 의미가 없는 자리인지를 알려줌\n",
    "## 이제 inputs가 준비되었다 -- GPU에 옮긴다 to(device) -- model.generate()\n",
    "## outputs가 나온다. 이 친구는 어떻게 생겼을까? 답변이 어디있는지 찾을 수 있는가??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7a8fd",
   "metadata": {
    "id": "fab7a8fd",
    "outputId": "745ceaf8-0d80-4c9f-a601-0b03db36f16d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False, # input뒤에다가 assistant를 붙일지 결정\n",
    "    tokenize=False,              # 결과를 토큰화할지 여부\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a54922",
   "metadata": {
    "id": "60a54922",
    "outputId": "5e084172-03c7-4611-83eb-7ea6f3243ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # input뒤에다가 assistant를 붙일지 결정\n",
    "    tokenize=False,              # 결과를 토큰화할지 여부\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c15aca",
   "metadata": {
    "id": "58c15aca",
    "outputId": "a6ce3e65-2769-470a-f66d-4cdf2a9e7083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d366e5",
   "metadata": {
    "id": "15d366e5"
   },
   "outputs": [],
   "source": [
    "model.generate(input_ids, attention_mask, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c2cd2",
   "metadata": {
    "id": "109c2cd2",
    "outputId": "8b7afcc0-bf74-4892-fc3e-47349cfa5344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\nWrite a poem on Hugging Face, the company<end_of_turn>\\n<start_of_turn>model\\nOkay, here’s a poem about Hugging Face, aiming to capture its essence – a community of AI, collaboration, and open-source creativity:\\n\\n**The Hugging Face Bloom**\\n\\nThe cloud expands, a gentle hue,\\nHugging Face, a vibrant view.\\nA haven built for code and']\n"
     ]
    }
   ],
   "source": [
    "# STEP4. 추론하기\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8f522",
   "metadata": {
    "id": "d8b8f522",
    "outputId": "8af26b48-4bcb-4ea7-9a8f-76805d8a9c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here’s a poem about Hugging Face, aiming to capture its essence – a community of AI, collaboration, and open-source creativity:\n",
      "\n",
      "**The Hugging Face Bloom**\n",
      "\n",
      "The cloud expands, a gentle hue,\n",
      "Hugging Face, a vibrant view.\n",
      "A haven built for code and\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d693d",
   "metadata": {
    "id": "903d693d",
    "outputId": "c1efbadc-67d7-449a-c61c-120a38b28127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ff73d",
   "metadata": {
    "id": "6b9ff73d",
    "outputId": "ec20887d-d998-4ef0-de28-ec446aeaae6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ou are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here’s a poem about Hugging Face, aiming to capture its essence – a community of AI, collaboration, and open-source creativity:\n",
      "\n",
      "**The Hugging Face Bloom**\n",
      "\n",
      "The cloud expands, a gentle hue,\n",
      "Hugging Face, a vibrant view.\n",
      "A haven built for code and\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # input뒤에다가 assistant를 붙일지 결정\n",
    "    tokenize=True,              # 결과를 토큰화할지 여부\n",
    "    return_dict=True,           # 결과를 딕셔너리로 반환하게 할 것인지의 여부\n",
    "    return_tensors=\"pt\",        # 결과를 파이토치 형식으로 반환할 것인지의 여부\n",
    ")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "print(outputs[0][input_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4aaaf",
   "metadata": {},
   "source": [
    "## 2) Gemma3 1B pt 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8393e6",
   "metadata": {},
   "source": [
    "### pipeline으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b5f29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-pt\", \n",
    "    device=device, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "output = pipe(\"Eiffel tower is located in\", max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a73e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the world’s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located'}]\n",
      "{'generated_text': 'Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the world’s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located'}\n",
      "Eiffel tower is located in the 7th district of ParisThe Eiffel Tower is located in the 7th district of Paris, FranceThe Eiffel Tower is the world’s tallest towerThe Eiffel Tower is the tallest tower in ParisEiffel Tower is located\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(output[0])\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8a3b6",
   "metadata": {},
   "source": [
    "### 모델로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# 모델 이름 설정\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. 모델, 코트나이저 불러오기\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. 입력 데이터 준비하기\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(device) # GPU 보내기\n",
    "\n",
    "# STEP4. 추론하기\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.decode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7f5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# 모델 이름 설정\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. 모델, 코트나이저 불러오기\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c960387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. 입력 데이터 준비하기\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU 보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5c5dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "torch.Size([1, 57])\n",
      "tensor([     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "           529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "           563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "         11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "        236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "          9079,    532,   7001], device='cuda:0')\n",
      "<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "# STEP4. 추론하기\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "print(outputs)\n",
    "print(outputs.shape)\n",
    "print(outputs[0])\n",
    "\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5927d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf22519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리\n",
    "# 1. 우리가 허깅페이스에서 LLM 모델을 사용하려고 할 때, 모델은 어떻게 불러오나요?\n",
    "# 2. instruct 모델이 있고, 그냥 pre-trained 모델이 있는데 모델에 input해야 할 데이터는 어떻게 생겼나요?\n",
    "# 3. input 해야할 데이터는 어떻게 만드나요?\n",
    "# 4. model에서 input data를 넣은 다음 나온 output은 어떻게 생겼나요?\n",
    "# 5. output을 텍스트로 바꾸려면 어떻게 해야 하나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22f99f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 239120, 238500, 238503, 239592, 237170, 110388, 237223,   5386,\n",
      "         103595, 236881,    107, 238136, 239484, 241845, 237456, 110388, 237223,\n",
      "           5386, 103595, 236881]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 응용해보기(업데이트 중입니다.......)\n",
    "prompt = [\n",
    "    \"남산타워는 어디에 있나요?<eos>\",\n",
    "    \"경복궁은 어디에 있나요?<eos>\"\n",
    "]\n",
    "prompt = \"남산타워는 어디에 있나요?\\n경복궁은 어디에 있나요?\"\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(\n",
    "    \"\".join(prompt), \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU 보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemme-3 1b it\n",
    "\n",
    "# 정리1. messages변수가 토크나이저를 만나 숫자로 바뀌는 과정은 어떤가?\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "## 모델에 바로 넣을 수 있는가????? No\n",
    "## messages 를 숫자 텐서로 바꿔야한다. ---- How?? tokenize.apply_chat_template(message)\n",
    "## 만약 tokenize=True를 하면, {\"input_ids\":, \"attention_mask\":}\n",
    "## ??? messages 는 문자가 아닌데 어떻게 숫자로 바꾸는거지?????? tokenize = False 해보면 알 수 있다.\n",
    "##  <bos><start....... bos 는 시작, eos 끝\n",
    "\n",
    "# 정리2. 우리가 예측을 하기 위해서는 어떤 데이터가 준비되어야 할까?\n",
    "## inputs = {\"input_ids\":\"A\", \"attention_mask\":\"B\"}\n",
    "\n",
    "# 정리3. 예측을 하는 과정에서 \"**\"는 왜 쓰는걸까?\n",
    "## def myfunc(input_ids, attention_mask, message=\"CCC\", verbose = Fase) \n",
    "## ...\n",
    "## myfunc(**inputs)의 의미는 myfunc(input_ids = \"A\", attention_mask=\"B\") 로 해주세요와 같다.\n",
    "\n",
    "# 정리4. output은 어떻게 나오는가?\n",
    "## outputs = model(inputs)\n",
    "## tensor - 숫자형\n",
    "\n",
    "# 정리5. ouutput에서 답변은 어떻게 추출할 수 있을까? \n",
    "# decode를 통해서 숫자 텐서를 텍스트로 바꿩한다.\n",
    "# tensor([데이터1, 데이터2 ...]) 인 경우, tokenizer.batch_decode\n",
    "# tensor(데이터1)인 경우, tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-3 1b pt\n",
    "\n",
    "# 정리\n",
    "# 1. 우리가 허깅페이스에서 LLM 모델을 사용하려고 할 때, 모델은 어떻게 불러오나요?\n",
    "## 모델 이름이 있는 사이트에 들어가면 \n",
    "## pipeline 함수로도 추론할 수 있고,\n",
    "## model을 직접 불러와서 할 수도 있다.----(별표) 파인튜닝: 내 데이터를 이미 학습되어 있는 모델에 적용시키기 위해 model \n",
    "\n",
    "# 2. instruct 모델이 있고, 그냥 pre-trained 모델이 있는데 모델에 input해야 할 데이터는 어떻게 생겼나요?\n",
    "## LLM 학습 방법 \n",
    "## 1) Pre-traind model 에서 \"새로운 정보\"를 학습시킨다. ex. gemma-3-1b-pt INPUT: \"프롬프트를 작성해주세요\" (str)\n",
    "## 2) Instruct model 에서 \"말하는 방식\"을 학습시킨다. ex. gemma-3-1b-it INPUT: 대화 [{\"role\":,\"content\":}] * 지시사항까지 학습한다.\n",
    "\n",
    "# 3. input 해야할 데이터는 어떻게 만드나요?\n",
    "## 텍스트 -- 텐서 -- model -- 텍스트\n",
    "##    tokenizer       tokenizer\n",
    "##    tokenize()      tokenizer.decode()\n",
    "##    tokenize.apply_chat_template()  \n",
    "\n",
    "# 4. model에서 input data를 넣은 다음 나온 output은 어떻게 생겼나요?\n",
    "## tenser \n",
    "\n",
    "# 5. output을 텍스트로 바꾸려면 어떻게 해야 하나요?\n",
    "# tokenizer.decode 시리즈 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c414730",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "poten_up09",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
